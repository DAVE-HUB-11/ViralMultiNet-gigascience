#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ„å»º DNABERT-2 å¾®è°ƒæ•°æ®é›†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ï¼š
- æ–°å¢k-merç¼–ç ã€åºåˆ—æˆªæ–­ã€é“¾ç‰¹å¼‚æ€§å¤„ç†
- å¢å¼ºæ•°æ®æ¸…æ´—å’Œé”™è¯¯å¤„ç†
"""

import pandas as pd
from Bio import SeqIO
import re
from tqdm import tqdm  # è¿›åº¦æ¡æ”¯æŒ

# â€”â€”â€”â€” é…ç½®å‚æ•° â€”â€”â€”â€”
K_MER_SIZE = 6  # é€‚é…DNABERT-2çš„k-merå¤§å°
MAX_SEQ_LENGTH = 512  # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆtokenæ•°ï¼‰
STRAND_AWARE = False  # æ˜¯å¦è€ƒè™‘é“¾ç‰¹å¼‚æ€§ï¼ˆTrueæ—¶è´Ÿé“¾å–åå‘äº’è¡¥ï¼‰

# â€”â€”â€”â€” æ–‡ä»¶è·¯å¾„ â€”â€”â€”â€”
MATCHES_CSV = r"C:\Users\10785\Desktop\matches.csv"
ANNOT_XLSX = r"C:\Users\10785\Desktop\annotations.csv.xlsx"
FNA_FILE = r"C:\Users\10785\Desktop\sraæ•°æ®\genes.fna"
OUTPUT_CSV = r"C:\Users\10785\Desktop\dnabert_dataset.csv"


def dna_to_kmers(sequence: str, k: int = K_MER_SIZE) -> str:
    """å°†DNAåºåˆ—è½¬æ¢ä¸ºk-meråˆ†è¯å­—ç¬¦ä¸²"""
    # æ¸…æ´—åºåˆ—ï¼šå»é™¤éATCGNå­—ç¬¦å¹¶è½¬å¤§å†™
    seq_clean = re.sub(r'[^ATCGN]', '', sequence.upper())
    # ç”Ÿæˆk-meråˆ—è¡¨
    kmers = [seq_clean[i:i + k] for i in range(len(seq_clean) - k + 1)]
    return ' '.join(kmers)


def process_sequence(raw_seq: str) -> str:
    """åºåˆ—å¤„ç†æµæ°´çº¿ï¼šæ¸…æ´—->k-mer->æˆªæ–­"""
    # é“¾ç‰¹å¼‚æ€§å¤„ç†ï¼ˆéœ€è¦GFFä¿¡æ¯æ—¶å¯æ‰©å±•ï¼‰
    if STRAND_AWARE and is_negative_strand(orf_id):  # éœ€å®ç°is_negative_strand
        raw_seq = str(Seq(raw_seq).reverse_complement())

    # k-merç¼–ç 
    kmer_seq = dna_to_kmers(raw_seq)

    # é•¿åº¦æˆªæ–­
    tokens = kmer_seq.split()
    if len(tokens) > MAX_SEQ_LENGTH:
        tokens = tokens[:MAX_SEQ_LENGTH]
    return ' '.join(tokens)


# â€”â€”â€”â€” Step 1: è¯»å– matches.csv â€”â€”â€”â€”
print("ğŸ”„ è¯»å– matches.csv ...")
matches_df = pd.read_csv(
    MATCHES_CSV,
    header=0,
    usecols=[0, 1],
    dtype={"subject_id": str}
).rename(columns={"query_id": "orf_id"})
print(f"âœ… å·²è¯»å– {len(matches_df):,} æ¡ ORFâ†’UniProt æ˜ å°„")

# â€”â€”â€”â€” Step 2: è¯»å–æ³¨é‡Šè¡¨å¹¶æ‰“æ ‡ç­¾ â€”â€”â€”â€”
print("\nğŸ”„ è¯»å–æ³¨é‡Šè¡¨ ...")
df = pd.read_excel(ANNOT_XLSX, engine="openpyxl")
print(df.columns)
anno_df = pd.read_excel(ANNOT_XLSX, engine="openpyxl")[["ids", "function"]]
anno_df = anno_df.dropna(subset=["ids", "function"]).astype({"ids": str})
anno_df = anno_df.rename(columns={"ids": "subject_id", "function": "annotation"})

# å¢å¼ºæ ‡ç­¾æ˜ å°„ï¼ˆæ”¯æŒä¸­è‹±æ–‡æ··åˆæ ‡æ³¨ï¼‰
LABEL_MAP = {
    0: ['é…¶', 'enzyme'],
    1: ['ç»“æ„è›‹ç™½', 'structural', 'capsid', 'spike'],
    2: ['è½¬è¿è›‹ç™½', 'transporter'],
    3: ['å…¶ä»–']  # é»˜è®¤ç±»åˆ«
}


def map_annotation_to_label(text: str) -> int:
    text = str(text).lower()
    for label, keywords in LABEL_MAP.items():
        if any(kw in text for kw in keywords):
            return label
    return 3  # é»˜è®¤ç±»åˆ«


anno_df["label"] = anno_df["annotation"].apply(map_annotation_to_label)
print(f"âœ… æ³¨é‡Šè¡¨å…±åŒ…å« {len(anno_df):,} æ¡æœ‰æ•ˆæ³¨é‡Š")

# â€”â€”â€”â€” Step 3: åˆå¹¶ ORFâ†’label æ˜ å°„ â€”â€”â€”â€”
print("\nğŸ”„ åˆå¹¶ ORF_ID å’Œæ ‡ç­¾ ...")
orf2label = matches_df.merge(
    anno_df,
    on="subject_id",
    how="inner"
)[["orf_id", "subject_id", "annotation", "label"]]
print(f"âœ… åˆå¹¶åå¾—åˆ° {len(orf2label):,} æ¡æœ‰æ ‡ç­¾ ORF (å»é‡å‰)")

# æŒ‰ORF_IDå»é‡ï¼ˆä¿ç•™æœ€åå‡ºç°çš„æ ‡ç­¾ï¼‰
orf2label = orf2label.drop_duplicates(subset=["orf_id"], keep='last')
print(f"âœ… å»é‡åæœ‰æ•ˆ ORF æ•°é‡: {len(orf2label):,}")

# â€”â€”â€”â€” Step 4: åŠ è½½å¹¶é¢„å¤„ç†åŸºå› åºåˆ— â€”â€”â€”â€”
print(f"\nğŸ”„ åŠ è½½å¹¶é¢„å¤„ç†åŸºå› åºåˆ— (k={K_MER_SIZE})...")
fna_dict = {}
seen_ids = set()
skipped = 0

for record in tqdm(SeqIO.parse(FNA_FILE, "fasta"), desc="Processing Genes"):
    seqid = record.id
    if seqid in seen_ids:
        skipped += 1
        continue
    seen_ids.add(seqid)

    # åºåˆ—é¢„å¤„ç†
    raw_seq = str(record.seq)
    processed_seq = process_sequence(raw_seq)

    # è¿‡æ»¤æ— æ•ˆåºåˆ—
    if len(processed_seq.replace(' ', '')) < K_MER_SIZE:  # çŸ­äºk-merçš„åºåˆ—
        skipped += 1
        continue

    fna_dict[seqid] = processed_seq

print(f"âœ… æˆåŠŸåŠ è½½ {len(fna_dict):,} æ¡å”¯ä¸€åºåˆ— | è·³è¿‡ {skipped} æ¡æ— æ•ˆåºåˆ—")

# â€”â€”â€”â€” Step 5: æ„å»ºæœ€ç»ˆæ•°æ®é›† â€”â€”â€”â€”
print("\nğŸ”„ æ„å»ºæ•°æ®é›†...")
sequences, labels, orf_ids, subject_ids, annotations = [], [], [], [], []
missing_count = 0

for _, row in tqdm(orf2label.iterrows(), total=len(orf2label), desc="Matching Sequences"):
    oid, sid, anno, lbl = row["orf_id"], row["subject_id"], row["annotation"], row["label"]
    if oid not in fna_dict:
        missing_count += 1
        continue
    sequences.append(fna_dict[oid])
    labels.append(lbl)
    orf_ids.append(oid)
    subject_ids.append(sid)
    annotations.append(anno)

print(f"âš ï¸  {missing_count} æ¡ ORF ç¼ºå°‘å¯¹åº”åºåˆ—")
print(f"âœ… æœ€ç»ˆæ•°æ®é›†åŒ…å« {len(sequences):,} æ¡æ ·æœ¬")

# â€”â€”â€”â€” Step 6: ä¿å­˜æ•°æ®é›† â€”â€”â€”â€”
print("\nğŸ’¾ å†™å…¥æ–‡ä»¶...")
out_df = pd.DataFrame({
    "orf_id": orf_ids,
    "subject_id": subject_ids,
    "annotation": annotations,
    "text": sequences,
    "label": labels
})

# æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
out_df['seq_length'] = out_df['text'].apply(lambda x: len(x.split()))
length_stats = out_df['seq_length'].describe()

out_df.to_csv(OUTPUT_CSV, index=False)
print(f"ğŸ‰ å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜åˆ°ï¼š{OUTPUT_CSV}")
print("\nğŸ“Š åºåˆ—é•¿åº¦ç»Ÿè®¡ï¼š")
print(f"å¹³å‡é•¿åº¦: {length_stats['mean']:.1f} Â± {length_stats['std']:.1f} tokens")
print(f"æœ€å°å€¼: {length_stats['min']} | ä¸­ä½æ•°: {length_stats['50%']} | æœ€å¤§å€¼: {length_stats['max']}")