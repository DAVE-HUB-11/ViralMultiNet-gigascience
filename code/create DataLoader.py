#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
build_dataloader.py

æ„å»º PyTorch Dataset + DataLoaderï¼š
- ä» dnabert_dataset_multiscale.csv è¯»å–å¤šå°ºåº¦åˆ†è¯åˆ—ï¼ˆé»˜è®¤ç”¨ 6-merï¼‰
- åŠ è½½å¯¹åº”çš„ Tokenizerï¼ˆå¦‚ tokenizer_6merï¼‰
- è¿”å› batch {"input_ids","attention_mask","labels"}
"""

import os
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import PreTrainedTokenizerFast, DataCollatorWithPadding

# â€”â€”â€”â€” é…ç½® â€”â€”â€”â€”
CSV_FILE     = r"C:\Users\10785\Desktop\dnabert_dataset_multiscale.csv"  # æ›´æ–°ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„
K            = 6  # é€‰æ‹© k-mer å°ºåº¦ï¼Œé»˜è®¤ä½¿ç”¨ 6-mer
COL_TEXT     = f"text_{K}mer"
COL_LABEL    = "label"
TOKENIZER_DIR= rf"C:\Users\10785\Desktop\tokenizer_{K}mer"  # æ›´æ–°ä¸ºä½ çš„ tokenizer æ–‡ä»¶è·¯å¾„
BATCH_SIZE   = 32
MAX_LENGTH   = 512  # è·Ÿå¾®è°ƒæ—¶ä¿æŒä¸€è‡´
SHUFFLE      = True
NUM_WORKERS  = 4
# â€”â€”â€”â€”â€”â€”â€”â€” #

class DNABertDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        seq = self.texts[idx]
        lbl = int(self.labels[idx])
        # åˆ†è¯
        encoded = self.tokenizer(
            seq,
            truncation=True,
            padding=False,  # è®© DataCollator ç»Ÿä¸€ padding
            max_length=self.max_length,
            return_attention_mask=True,
        )
        return {
            "input_ids": torch.tensor(encoded["input_ids"], dtype=torch.long),
            "attention_mask": torch.tensor(encoded["attention_mask"], dtype=torch.long),
            "labels": torch.tensor(lbl, dtype=torch.long),
        }

def collate_fn(batch):
    """
    ä½¿ç”¨ HuggingFace çš„ DataCollatorWithPadding ç®€åŒ– padding
    """
    collator = DataCollatorWithPadding(tokenizer=tokenizer, padding="longest")
    return collator(batch)

if __name__ == "__main__":
    # 1. è¯»å–å¤šå°ºåº¦ CSV
    df = pd.read_csv(CSV_FILE, usecols=[COL_TEXT, COL_LABEL])
    print(f"ğŸ“Š å…±åŠ è½½ {len(df):,} æ¡æ ·æœ¬ï¼Œç¤ºä¾‹ï¼š")
    print(df.head())

    # 2. åŠ è½½å¯¹åº” tokenizer
    if not os.path.isdir(TOKENIZER_DIR):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° Tokenizer ç›®å½•ï¼š{TOKENIZER_DIR}")
    tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKENIZER_DIR)
    print(f"ğŸ›  å·²åŠ è½½ Tokenizerï¼š{TOKENIZER_DIR}")

    # 3. æ„å»º Dataset
    dataset = DNABertDataset(
        texts=df[COL_TEXT].tolist(),
        labels=df[COL_LABEL].tolist(),
        tokenizer=tokenizer,
        max_length=MAX_LENGTH
    )

    # 4. æ„å»º DataLoader
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding="longest")
    loader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=SHUFFLE,
        num_workers=NUM_WORKERS,
        collate_fn=data_collator
    )

    # 5. ç®€å•æµ‹è¯•ä¸€ä¸ª batch
    batch = next(iter(loader))
    print("\nâ–¶ æµ‹è¯•ä¸€ä¸ª batch è¾“å‡ºï¼š")
    print("input_ids:", batch["input_ids"].shape)
    print("attention_mask:", batch["attention_mask"].shape)
    print("labels:", batch["labels"].shape)

    # 6. æš´éœ² loader å’Œ dataset ä¾›åç»­ä½¿ç”¨
    #    ä½ å¯ä»¥åœ¨è®­ç»ƒè„šæœ¬é‡Œè¿™æ ·åšï¼š
    #    from build_dataloader import loader as train_loader
